# ğŸ”¬ AHP ê³ ê¸‰ ë¶„ì„ ì•Œê³ ë¦¬ì¦˜ ìƒì„¸ ì„¤ê³„
**ì‘ì„±ì¼**: 2024-11-29  
**ì‘ì„±ì**: Claude Opus 4.1  
**ë²„ì „**: 1.0

## 1. ì‹œìŠ¤í…œ ê°œìš”

### 1.1 ëª©ì 
AHP ì˜ì‚¬ê²°ì • ê³¼ì •ì—ì„œ ê²°ê³¼ì˜ ì‹ ë¢°ì„±ê³¼ íƒ€ë‹¹ì„±ì„ ë†’ì´ê¸° ìœ„í•œ ê³ ê¸‰ ë¶„ì„ ê¸°ë²• ì œê³µ. ë¯¼ê°ë„ ë¶„ì„, What-if ì‹œë‚˜ë¦¬ì˜¤, Monte Carlo ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•œ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™” ë° ì˜ì‚¬ê²°ì • ì§€ì›

### 1.2 í•µì‹¬ ê¸°ëŠ¥
- **ë¯¼ê°ë„ ë¶„ì„**: ì…ë ¥ ë³€í™”ì— ëŒ€í•œ ì¶œë ¥ ë³€í™” ì¸¡ì •
- **What-if ì‹œë‚˜ë¦¬ì˜¤**: ê°€ì • ë³€ê²½ì— ë”°ë¥¸ ê²°ê³¼ ì˜ˆì¸¡
- **Monte Carlo ì‹œë®¬ë ˆì´ì…˜**: í™•ë¥ ì  ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§
- **ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ ë¶„ì„**: ê²°ê³¼ì˜ ì•ˆì •ì„± ê²€ì¦
- **ìˆœìœ„ ì—­ì „ ë¶„ì„**: ëŒ€ì•ˆ ìˆœìœ„ ë³€ê²½ ì„ê³„ì  ê³„ì‚°

---

## 2. ë¯¼ê°ë„ ë¶„ì„ ì—”ì§„

### 2.1 êµ­ì†Œ ë¯¼ê°ë„ ë¶„ì„ (Local Sensitivity Analysis)

```python
import numpy as np
from typing import Dict, List, Tuple, Optional
from scipy.optimize import minimize_scalar
import pandas as pd

class LocalSensitivityAnalyzer:
    """êµ­ì†Œ ë¯¼ê°ë„ ë¶„ì„ ì—”ì§„"""
    
    def __init__(self, ahp_model):
        self.model = ahp_model
        self.n_criteria = len(ahp_model.criteria)
        self.n_alternatives = len(ahp_model.alternatives)
        self.base_weights = ahp_model.get_criteria_weights()
        self.base_scores = ahp_model.get_alternative_scores()
    
    def calculate_elasticity(
        self, 
        criterion_index: int, 
        delta: float = 0.001
    ) -> Dict[str, float]:
        """
        íƒ„ë ¥ì„± ê³„ì‚°: ê°€ì¤‘ì¹˜ 1% ë³€í™”ì— ëŒ€í•œ ì ìˆ˜ ë³€í™”ìœ¨
        
        E_ij = (âˆ‚S_j / âˆ‚w_i) * (w_i / S_j)
        
        Parameters:
            criterion_index: ë¶„ì„í•  ê¸°ì¤€ ì¸ë±ìŠ¤
            delta: ë¯¸ì†Œ ë³€í™”ëŸ‰ (ê¸°ë³¸ 0.1%)
        
        Returns:
            ê° ëŒ€ì•ˆë³„ íƒ„ë ¥ì„± ê°’
        """
        
        elasticities = {}
        original_weight = self.base_weights[criterion_index]
        
        # ê°€ì¤‘ì¹˜ ì¦ê°€ ì‹œ ì ìˆ˜ ê³„ì‚°
        weights_up = self.base_weights.copy()
        weights_up[criterion_index] *= (1 + delta)
        weights_up = self._normalize_weights(weights_up)
        scores_up = self._calculate_scores(weights_up)
        
        # ê°€ì¤‘ì¹˜ ê°ì†Œ ì‹œ ì ìˆ˜ ê³„ì‚°
        weights_down = self.base_weights.copy()
        weights_down[criterion_index] *= (1 - delta)
        weights_down = self._normalize_weights(weights_down)
        scores_down = self._calculate_scores(weights_down)
        
        # ì¤‘ì‹¬ì°¨ë¶„ë²•ìœ¼ë¡œ ë¯¸ë¶„ ê·¼ì‚¬
        for j, alt_name in enumerate(self.model.alternatives):
            derivative = (scores_up[j] - scores_down[j]) / (2 * delta * original_weight)
            elasticity = derivative * (original_weight / self.base_scores[j])
            elasticities[alt_name] = elasticity
        
        return elasticities
    
    def find_rank_reversal_points(
        self, 
        criterion_index: int
    ) -> List[Dict[str, any]]:
        """
        ìˆœìœ„ ì—­ì „ í¬ì¸íŠ¸ ì°¾ê¸°
        
        íŠ¹ì • ê¸°ì¤€ì˜ ê°€ì¤‘ì¹˜ê°€ ë³€í•  ë•Œ ëŒ€ì•ˆ ìˆœìœ„ê°€ ë°”ë€ŒëŠ” ì„ê³„ì  ê³„ì‚°
        
        Parameters:
            criterion_index: ë¶„ì„í•  ê¸°ì¤€ ì¸ë±ìŠ¤
        
        Returns:
            ìˆœìœ„ ì—­ì „ì´ ë°œìƒí•˜ëŠ” ê°€ì¤‘ì¹˜ ê°’ê³¼ ê´€ë ¨ ì •ë³´
        """
        
        reversal_points = []
        original_ranking = np.argsort(-self.base_scores)
        
        # ê°€ì¤‘ì¹˜ ë²”ìœ„ [0, 1]ì—ì„œ íƒìƒ‰
        for weight in np.linspace(0, 1, 1000):
            # ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë“¤ì„ ë¹„ë¡€ì ìœ¼ë¡œ ì¡°ì •
            new_weights = self._adjust_weight_proportionally(
                criterion_index, 
                weight
            )
            
            new_scores = self._calculate_scores(new_weights)
            new_ranking = np.argsort(-new_scores)
            
            # ìˆœìœ„ ë³€ê²½ ê°ì§€
            if not np.array_equal(original_ranking, new_ranking):
                reversal_info = {
                    'criterion_weight': weight,
                    'original_weight': self.base_weights[criterion_index],
                    'change_ratio': weight / self.base_weights[criterion_index],
                    'old_ranking': original_ranking.tolist(),
                    'new_ranking': new_ranking.tolist(),
                    'affected_alternatives': self._get_affected_alternatives(
                        original_ranking, 
                        new_ranking
                    )
                }
                reversal_points.append(reversal_info)
                
                # ì—°ì†ëœ ì¤‘ë³µ ì œê±°
                if len(reversal_points) > 1:
                    if reversal_points[-1]['new_ranking'] == reversal_points[-2]['new_ranking']:
                        reversal_points.pop()
        
        return self._consolidate_reversal_points(reversal_points)
    
    def calculate_gradient_vector(self) -> np.ndarray:
        """
        ì „ì²´ ê¸°ì¤€ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ë²¡í„° ê³„ì‚°
        
        âˆ‡S = [âˆ‚S/âˆ‚w_1, âˆ‚S/âˆ‚w_2, ..., âˆ‚S/âˆ‚w_n]
        
        Returns:
            ê° ê¸°ì¤€ë³„ ë¯¼ê°ë„ ê·¸ë˜ë””ì–¸íŠ¸
        """
        
        gradient = np.zeros((self.n_alternatives, self.n_criteria))
        
        for i in range(self.n_criteria):
            for j in range(self.n_alternatives):
                # í¸ë¯¸ë¶„ ê³„ì‚°
                gradient[j, i] = self._partial_derivative(i, j)
        
        return gradient
    
    def _partial_derivative(
        self, 
        criterion_idx: int, 
        alternative_idx: int,
        h: float = 1e-5
    ) -> float:
        """ì¤‘ì‹¬ì°¨ë¶„ë²•ì„ ì´ìš©í•œ í¸ë¯¸ë¶„ ê³„ì‚°"""
        
        weights_plus = self.base_weights.copy()
        weights_plus[criterion_idx] += h
        weights_plus = self._normalize_weights(weights_plus)
        scores_plus = self._calculate_scores(weights_plus)
        
        weights_minus = self.base_weights.copy()
        weights_minus[criterion_idx] -= h
        weights_minus = self._normalize_weights(weights_minus)
        scores_minus = self._calculate_scores(weights_minus)
        
        return (scores_plus[alternative_idx] - scores_minus[alternative_idx]) / (2 * h)
    
    def _normalize_weights(self, weights: np.ndarray) -> np.ndarray:
        """ê°€ì¤‘ì¹˜ ì •ê·œí™”"""
        return weights / weights.sum()
    
    def _calculate_scores(self, weights: np.ndarray) -> np.ndarray:
        """ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ë¡œ ì ìˆ˜ ì¬ê³„ì‚°"""
        performance_matrix = self.model.get_performance_matrix()
        return performance_matrix @ weights
```

### 2.2 ì „ì—­ ë¯¼ê°ë„ ë¶„ì„ (Global Sensitivity Analysis)

```python
from scipy.stats import norm, uniform
from SALib.sample import saltelli
from SALib.analyze import sobol

class GlobalSensitivityAnalyzer:
    """ì „ì—­ ë¯¼ê°ë„ ë¶„ì„ - Sobol ë°©ë²•"""
    
    def __init__(self, ahp_model):
        self.model = ahp_model
        self.n_criteria = len(ahp_model.criteria)
        
    def sobol_analysis(
        self, 
        n_samples: int = 10000,
        confidence_level: float = 0.95
    ) -> Dict[str, np.ndarray]:
        """
        Sobol ë¯¼ê°ë„ ì§€ìˆ˜ ê³„ì‚°
        
        - First-order indices: ê°œë³„ ë§¤ê°œë³€ìˆ˜ì˜ ì§ì ‘ íš¨ê³¼
        - Total-order indices: ìƒí˜¸ì‘ìš©ì„ í¬í•¨í•œ ì „ì²´ íš¨ê³¼
        
        Parameters:
            n_samples: ìƒ˜í”Œ ìˆ˜
            confidence_level: ì‹ ë¢° êµ¬ê°„
        
        Returns:
            Sobol ì§€ìˆ˜ì™€ ì‹ ë¢° êµ¬ê°„
        """
        
        # ë¬¸ì œ ì •ì˜
        problem = {
            'num_vars': self.n_criteria,
            'names': [f'w_{i}' for i in range(self.n_criteria)],
            'bounds': [[0.0, 1.0] for _ in range(self.n_criteria)]
        }
        
        # Saltelli ìƒ˜í”Œë§
        param_values = saltelli.sample(problem, n_samples)
        
        # ê° ìƒ˜í”Œì— ëŒ€í•´ ëª¨ë¸ í‰ê°€
        Y = np.zeros((param_values.shape[0], self.n_alternatives))
        
        for i, params in enumerate(param_values):
            # ê°€ì¤‘ì¹˜ ì •ê·œí™”
            normalized_weights = params / params.sum()
            scores = self._evaluate_model(normalized_weights)
            Y[i, :] = scores
        
        # Sobol ë¶„ì„
        sobol_indices = {}
        
        for alt_idx in range(self.n_alternatives):
            Si = sobol.analyze(problem, Y[:, alt_idx])
            
            sobol_indices[f'alternative_{alt_idx}'] = {
                'S1': Si['S1'],  # First-order indices
                'S1_conf': Si['S1_conf'],  # Confidence intervals
                'ST': Si['ST'],  # Total-order indices
                'ST_conf': Si['ST_conf'],
                'S2': Si['S2']  # Second-order indices (interactions)
            }
        
        return sobol_indices
    
    def morris_screening(
        self,
        n_trajectories: int = 100,
        n_levels: int = 4
    ) -> Dict[str, float]:
        """
        Morris ë°©ë²•ì„ ì´ìš©í•œ ìŠ¤í¬ë¦¬ë‹
        
        ê³„ì‚° ë¹„ìš©ì´ ì ì€ ë¯¼ê°ë„ ìŠ¤í¬ë¦¬ë‹ ë°©ë²•
        
        Returns:
            ê° ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ì´ˆ íš¨ê³¼ í‰ê· ê³¼ í‘œì¤€í¸ì°¨
        """
        
        from SALib.sample import morris as morris_sample
        from SALib.analyze import morris as morris_analyze
        
        problem = {
            'num_vars': self.n_criteria,
            'names': [f'criterion_{i}' for i in range(self.n_criteria)],
            'bounds': [[0.0, 1.0] for _ in range(self.n_criteria)]
        }
        
        # Morris ìƒ˜í”Œë§
        X = morris_sample.sample(
            problem, 
            n_trajectories, 
            num_levels=n_levels
        )
        
        # ëª¨ë¸ í‰ê°€
        Y = np.array([self._evaluate_model_scalar(x) for x in X])
        
        # Morris ë¶„ì„
        Si = morris_analyze.analyze(
            problem, 
            X, 
            Y, 
            conf_level=0.95,
            num_resamples=1000
        )
        
        return {
            'mu': Si['mu'],  # í‰ê·  ê¸°ì´ˆ íš¨ê³¼
            'mu_star': Si['mu_star'],  # í‰ê·  ì ˆëŒ€ ê¸°ì´ˆ íš¨ê³¼
            'sigma': Si['sigma'],  # í‘œì¤€í¸ì°¨
            'mu_star_conf': Si['mu_star_conf']  # ì‹ ë¢°êµ¬ê°„
        }
```

---

## 3. What-if ì‹œë‚˜ë¦¬ì˜¤ ì—”ì§„

### 3.1 ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ë° ê´€ë¦¬

```python
from dataclasses import dataclass
from enum import Enum
import itertools

class ScenarioType(Enum):
    """ì‹œë‚˜ë¦¬ì˜¤ ìœ í˜•"""
    OPTIMISTIC = "optimistic"
    PESSIMISTIC = "pessimistic"
    MOST_LIKELY = "most_likely"
    CUSTOM = "custom"
    EXTREME = "extreme"

@dataclass
class Scenario:
    """ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    name: str
    description: str
    type: ScenarioType
    weight_changes: Dict[str, float]
    judgment_changes: Dict[Tuple[str, str], float]
    probability: float = 1.0
    created_at: datetime = None
    
class WhatIfScenarioEngine:
    """What-if ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ì—”ì§„"""
    
    def __init__(self, ahp_model):
        self.model = ahp_model
        self.base_state = self._capture_model_state()
        self.scenarios = {}
        
    def create_scenario(
        self,
        name: str,
        scenario_type: ScenarioType,
        changes: Dict[str, any]
    ) -> Scenario:
        """
        ìƒˆ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        
        Parameters:
            name: ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„
            scenario_type: ì‹œë‚˜ë¦¬ì˜¤ ìœ í˜•
            changes: ë³€ê²½ ì‚¬í•­ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ìƒì„±ëœ ì‹œë‚˜ë¦¬ì˜¤ ê°ì²´
        """
        
        scenario = Scenario(
            id=self._generate_scenario_id(),
            name=name,
            description=f"{scenario_type.value} scenario",
            type=scenario_type,
            weight_changes=changes.get('weights', {}),
            judgment_changes=changes.get('judgments', {}),
            created_at=datetime.now()
        )
        
        self.scenarios[scenario.id] = scenario
        return scenario
    
    def generate_automatic_scenarios(self) -> List[Scenario]:
        """
        ìë™ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        
        Returns:
            ìƒì„±ëœ ì‹œë‚˜ë¦¬ì˜¤ ë¦¬ìŠ¤íŠ¸
        """
        
        scenarios = []
        
        # 1. ë‚™ê´€ì  ì‹œë‚˜ë¦¬ì˜¤ (ìµœê³  í‰ê°€ ê¸°ì¤€ ê°•ì¡°)
        optimistic = self._generate_optimistic_scenario()
        scenarios.append(optimistic)
        
        # 2. ë¹„ê´€ì  ì‹œë‚˜ë¦¬ì˜¤ (ìœ„í—˜ ìš”ì†Œ ê°•ì¡°)
        pessimistic = self._generate_pessimistic_scenario()
        scenarios.append(pessimistic)
        
        # 3. ê· í˜• ì‹œë‚˜ë¦¬ì˜¤ (ëª¨ë“  ê¸°ì¤€ ë™ì¼ ê°€ì¤‘ì¹˜)
        balanced = self._generate_balanced_scenario()
        scenarios.append(balanced)
        
        # 4. ê·¹ë‹¨ ì‹œë‚˜ë¦¬ì˜¤ (ê° ê¸°ì¤€ì„ ê·¹ëŒ€í™”)
        for i, criterion in enumerate(self.model.criteria):
            extreme = self._generate_extreme_scenario(i)
            scenarios.append(extreme)
        
        return scenarios
    
    def evaluate_scenario(
        self,
        scenario: Scenario,
        return_details: bool = True
    ) -> Dict[str, any]:
        """
        ì‹œë‚˜ë¦¬ì˜¤ í‰ê°€
        
        Parameters:
            scenario: í‰ê°€í•  ì‹œë‚˜ë¦¬ì˜¤
            return_details: ìƒì„¸ ì •ë³´ ë°˜í™˜ ì—¬ë¶€
        
        Returns:
            ì‹œë‚˜ë¦¬ì˜¤ í‰ê°€ ê²°ê³¼
        """
        
        # ëª¨ë¸ ìƒíƒœ ì„ì‹œ ì €ì¥
        temp_state = self._capture_model_state()
        
        # ì‹œë‚˜ë¦¬ì˜¤ ì ìš©
        self._apply_scenario_changes(scenario)
        
        # ê²°ê³¼ ê³„ì‚°
        results = {
            'scenario_id': scenario.id,
            'scenario_name': scenario.name,
            'final_scores': self.model.get_alternative_scores(),
            'ranking': self.model.get_ranking(),
            'consistency_ratios': self.model.get_all_consistency_ratios()
        }
        
        if return_details:
            results.update({
                'weight_changes': scenario.weight_changes,
                'score_changes': self._calculate_score_changes(),
                'rank_changes': self._calculate_rank_changes(),
                'robustness_index': self._calculate_robustness_index()
            })
        
        # ëª¨ë¸ ìƒíƒœ ë³µì›
        self._restore_model_state(temp_state)
        
        return results
    
    def compare_scenarios(
        self,
        scenario_ids: List[str]
    ) -> pd.DataFrame:
        """
        ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ
        
        Parameters:
            scenario_ids: ë¹„êµí•  ì‹œë‚˜ë¦¬ì˜¤ ID ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ë¹„êµ ê²°ê³¼ DataFrame
        """
        
        comparison_data = []
        
        for scenario_id in scenario_ids:
            if scenario_id not in self.scenarios:
                continue
                
            scenario = self.scenarios[scenario_id]
            results = self.evaluate_scenario(scenario)
            
            for alt_idx, alt_name in enumerate(self.model.alternatives):
                comparison_data.append({
                    'Scenario': scenario.name,
                    'Alternative': alt_name,
                    'Score': results['final_scores'][alt_idx],
                    'Rank': results['ranking'][alt_idx],
                    'Score_Change': results['score_changes'][alt_idx],
                    'Rank_Change': results['rank_changes'][alt_idx]
                })
        
        df = pd.DataFrame(comparison_data)
        return df.pivot_table(
            index='Alternative',
            columns='Scenario',
            values=['Score', 'Rank'],
            aggfunc='first'
        )
    
    def scenario_clustering(
        self,
        n_clusters: int = 3
    ) -> Dict[str, List[str]]:
        """
        ì‹œë‚˜ë¦¬ì˜¤ í´ëŸ¬ìŠ¤í„°ë§
        
        ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë“¤ì„ ê·¸ë£¹í™”
        
        Parameters:
            n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜
        
        Returns:
            í´ëŸ¬ìŠ¤í„°ë³„ ì‹œë‚˜ë¦¬ì˜¤ ID ë¦¬ìŠ¤íŠ¸
        """
        
        from sklearn.cluster import KMeans
        from sklearn.preprocessing import StandardScaler
        
        # ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ì˜ ê²°ê³¼ ìˆ˜ì§‘
        scenario_features = []
        scenario_ids = []
        
        for scenario_id, scenario in self.scenarios.items():
            results = self.evaluate_scenario(scenario, return_details=False)
            features = np.concatenate([
                results['final_scores'],
                results['ranking'].astype(float)
            ])
            scenario_features.append(features)
            scenario_ids.append(scenario_id)
        
        # ì •ê·œí™”
        X = StandardScaler().fit_transform(scenario_features)
        
        # í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(X)
        
        # ê²°ê³¼ ì •ë¦¬
        cluster_dict = {f'cluster_{i}': [] for i in range(n_clusters)}
        
        for scenario_id, cluster in zip(scenario_ids, clusters):
            cluster_dict[f'cluster_{cluster}'].append(scenario_id)
        
        return cluster_dict
```

### 3.2 ì‹œë‚˜ë¦¬ì˜¤ ìµœì í™”

```python
from scipy.optimize import differential_evolution

class ScenarioOptimizer:
    """ì‹œë‚˜ë¦¬ì˜¤ ìµœì í™” ì—”ì§„"""
    
    def __init__(self, ahp_model):
        self.model = ahp_model
        
    def find_optimal_scenario(
        self,
        objective: str,
        constraints: Dict[str, any] = None
    ) -> Scenario:
        """
        ëª©ì  í•¨ìˆ˜ë¥¼ ìµœì í™”í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ ì°¾ê¸°
        
        Parameters:
            objective: ìµœì í™” ëª©í‘œ ('max_discrimination', 'min_variance', ë“±)
            constraints: ì œì•½ ì¡°ê±´
        
        Returns:
            ìµœì  ì‹œë‚˜ë¦¬ì˜¤
        """
        
        # ëª©ì  í•¨ìˆ˜ ì •ì˜
        if objective == 'max_discrimination':
            objective_func = self._maximize_discrimination
        elif objective == 'min_variance':
            objective_func = self._minimize_variance
        elif objective == 'max_robustness':
            objective_func = self._maximize_robustness
        else:
            raise ValueError(f"Unknown objective: {objective}")
        
        # ìµœì í™” ë¬¸ì œ ì„¤ì •
        bounds = [(0.01, 1.0) for _ in range(self.model.n_criteria)]
        
        # ì°¨ë¶„ ì§„í™” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœì í™”
        result = differential_evolution(
            objective_func,
            bounds,
            constraints=constraints,
            maxiter=1000,
            popsize=50,
            seed=42
        )
        
        # ìµœì  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        optimal_weights = result.x / result.x.sum()
        
        scenario = Scenario(
            id=f"optimal_{objective}",
            name=f"Optimal scenario for {objective}",
            description=f"Optimized to {objective}",
            type=ScenarioType.CUSTOM,
            weight_changes={
                f'criterion_{i}': weight 
                for i, weight in enumerate(optimal_weights)
            },
            judgment_changes={}
        )
        
        return scenario
    
    def _maximize_discrimination(self, weights: np.ndarray) -> float:
        """ëŒ€ì•ˆ ê°„ ì°¨ë³„ì„± ìµœëŒ€í™”"""
        weights = weights / weights.sum()
        scores = self.model.calculate_scores_with_weights(weights)
        return -np.std(scores)  # ìµœëŒ€í™”ë¥¼ ìœ„í•´ ìŒìˆ˜
    
    def _minimize_variance(self, weights: np.ndarray) -> float:
        """ì ìˆ˜ ë¶„ì‚° ìµœì†Œí™”"""
        weights = weights / weights.sum()
        scores = self.model.calculate_scores_with_weights(weights)
        return np.var(scores)
    
    def _maximize_robustness(self, weights: np.ndarray) -> float:
        """ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ ìµœëŒ€í™”"""
        weights = weights / weights.sum()
        
        # ê°€ì¤‘ì¹˜ì— ë…¸ì´ì¦ˆ ì¶”ê°€í•˜ì—¬ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
        n_perturbations = 100
        rank_stability = 0
        
        base_ranking = self.model.get_ranking_with_weights(weights)
        
        for _ in range(n_perturbations):
            noise = np.random.normal(0, 0.01, len(weights))
            perturbed_weights = weights + noise
            perturbed_weights = np.clip(perturbed_weights, 0.01, 1)
            perturbed_weights = perturbed_weights / perturbed_weights.sum()
            
            perturbed_ranking = self.model.get_ranking_with_weights(perturbed_weights)
            
            # Kendall's tau ìƒê´€ê³„ìˆ˜
            from scipy.stats import kendalltau
            tau, _ = kendalltau(base_ranking, perturbed_ranking)
            rank_stability += tau
        
        return -rank_stability / n_perturbations  # ìµœëŒ€í™”ë¥¼ ìœ„í•´ ìŒìˆ˜
```

---

## 4. Monte Carlo ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„

### 4.1 í™•ë¥ ì  AHP ëª¨ë¸

```python
from scipy.stats import beta, truncnorm, dirichlet
import multiprocessing as mp
from functools import partial

class MonteCarloAHPEngine:
    """Monte Carlo ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ AHP ë¶„ì„"""
    
    def __init__(self, ahp_model, n_cores: int = None):
        self.model = ahp_model
        self.n_cores = n_cores or mp.cpu_count()
        self.simulation_results = []
        
    def run_simulation(
        self,
        n_iterations: int = 10000,
        uncertainty_model: str = 'uniform',
        uncertainty_level: float = 0.1,
        parallel: bool = True
    ) -> Dict[str, any]:
        """
        Monte Carlo ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        
        Parameters:
            n_iterations: ì‹œë®¬ë ˆì´ì…˜ ë°˜ë³µ íšŸìˆ˜
            uncertainty_model: ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ ('uniform', 'normal', 'beta')
            uncertainty_level: ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€ (0-1)
            parallel: ë³‘ë ¬ ì²˜ë¦¬ ì—¬ë¶€
        
        Returns:
            ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ í†µê³„
        """
        
        if parallel and self.n_cores > 1:
            results = self._run_parallel_simulation(
                n_iterations,
                uncertainty_model,
                uncertainty_level
            )
        else:
            results = self._run_sequential_simulation(
                n_iterations,
                uncertainty_model,
                uncertainty_level
            )
        
        self.simulation_results = results
        
        return self._analyze_simulation_results(results)
    
    def _run_sequential_simulation(
        self,
        n_iterations: int,
        uncertainty_model: str,
        uncertainty_level: float
    ) -> List[Dict]:
        """ìˆœì°¨ì  ì‹œë®¬ë ˆì´ì…˜"""
        
        results = []
        
        for i in range(n_iterations):
            # ë¶ˆí™•ì‹¤ì„±ì´ ì¶”ê°€ëœ íŒë‹¨ í–‰ë ¬ ìƒì„±
            perturbed_matrices = self._generate_perturbed_matrices(
                uncertainty_model,
                uncertainty_level
            )
            
            # AHP ê³„ì‚°
            iteration_result = self._calculate_ahp_with_matrices(perturbed_matrices)
            iteration_result['iteration'] = i
            
            results.append(iteration_result)
        
        return results
    
    def _run_parallel_simulation(
        self,
        n_iterations: int,
        uncertainty_model: str,
        uncertainty_level: float
    ) -> List[Dict]:
        """ë³‘ë ¬ ì‹œë®¬ë ˆì´ì…˜"""
        
        # ì‘ì—… í•¨ìˆ˜ ì •ì˜
        worker_func = partial(
            self._simulation_worker,
            uncertainty_model=uncertainty_model,
            uncertainty_level=uncertainty_level
        )
        
        # ë³‘ë ¬ ì²˜ë¦¬
        with mp.Pool(processes=self.n_cores) as pool:
            results = pool.map(worker_func, range(n_iterations))
        
        return results
    
    def _generate_perturbed_matrices(
        self,
        uncertainty_model: str,
        uncertainty_level: float
    ) -> Dict[str, np.ndarray]:
        """
        ë¶ˆí™•ì‹¤ì„±ì´ ì¶”ê°€ëœ íŒë‹¨ í–‰ë ¬ ìƒì„±
        
        Parameters:
            uncertainty_model: ë¶ˆí™•ì‹¤ì„± ëª¨ë¸
            uncertainty_level: ë¶ˆí™•ì‹¤ì„± ìˆ˜ì¤€
        
        Returns:
            ì„­ë™ëœ íŒë‹¨ í–‰ë ¬ë“¤
        """
        
        perturbed = {}
        
        for matrix_name, original_matrix in self.model.judgment_matrices.items():
            n = original_matrix.shape[0]
            perturbed_matrix = np.eye(n)
            
            for i in range(n):
                for j in range(i+1, n):
                    original_value = original_matrix[i, j]
                    
                    # ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ì— ë”°ë¥¸ ì„­ë™
                    if uncertainty_model == 'uniform':
                        # ê· ë“± ë¶„í¬
                        lower = original_value * (1 - uncertainty_level)
                        upper = original_value * (1 + uncertainty_level)
                        perturbed_value = np.random.uniform(lower, upper)
                        
                    elif uncertainty_model == 'normal':
                        # ì •ê·œ ë¶„í¬ (truncated)
                        std = original_value * uncertainty_level
                        perturbed_value = truncnorm.rvs(
                            a=(1/9 - original_value) / std,
                            b=(9 - original_value) / std,
                            loc=original_value,
                            scale=std
                        )
                        
                    elif uncertainty_model == 'beta':
                        # ë² íƒ€ ë¶„í¬ (skewed uncertainty)
                        # ì›ë˜ ê°’ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ í™•ë¥ 
                        alpha = 2 + (1 / uncertainty_level)
                        beta_param = alpha
                        
                        # [1/9, 9] ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§
                        normalized_original = (original_value - 1/9) / (9 - 1/9)
                        perturbed_normalized = beta.rvs(alpha, beta_param)
                        perturbed_value = 1/9 + perturbed_normalized * (9 - 1/9)
                    
                    else:
                        perturbed_value = original_value
                    
                    # Saaty ìŠ¤ì¼€ì¼ ë²”ìœ„ ì œí•œ
                    perturbed_value = np.clip(perturbed_value, 1/9, 9)
                    
                    perturbed_matrix[i, j] = perturbed_value
                    perturbed_matrix[j, i] = 1 / perturbed_value
            
            perturbed[matrix_name] = perturbed_matrix
        
        return perturbed
    
    def _analyze_simulation_results(
        self,
        results: List[Dict]
    ) -> Dict[str, any]:
        """
        ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ë¶„ì„
        
        Returns:
            í†µê³„ì  ë¶„ì„ ê²°ê³¼
        """
        
        n_alternatives = len(self.model.alternatives)
        n_iterations = len(results)
        
        # ì ìˆ˜ í–‰ë ¬ ìƒì„±
        scores_matrix = np.zeros((n_iterations, n_alternatives))
        rankings_matrix = np.zeros((n_iterations, n_alternatives), dtype=int)
        
        for i, result in enumerate(results):
            scores_matrix[i, :] = result['scores']
            rankings_matrix[i, :] = result['ranking']
        
        analysis = {
            'mean_scores': np.mean(scores_matrix, axis=0),
            'std_scores': np.std(scores_matrix, axis=0),
            'confidence_intervals': self._calculate_confidence_intervals(scores_matrix),
            'rank_probabilities': self._calculate_rank_probabilities(rankings_matrix),
            'rank_stability': self._calculate_rank_stability(rankings_matrix),
            'dominance_matrix': self._calculate_dominance_matrix(scores_matrix),
            'value_of_information': self._calculate_voi(scores_matrix)
        }
        
        return analysis
    
    def _calculate_confidence_intervals(
        self,
        scores_matrix: np.ndarray,
        confidence_level: float = 0.95
    ) -> Dict[str, Tuple[float, float]]:
        """ì‹ ë¢° êµ¬ê°„ ê³„ì‚°"""
        
        from scipy import stats
        
        confidence_intervals = {}
        alpha = 1 - confidence_level
        
        for i, alt_name in enumerate(self.model.alternatives):
            scores = scores_matrix[:, i]
            
            # ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„
            bootstrap_samples = []
            for _ in range(1000):
                sample = np.random.choice(scores, size=len(scores), replace=True)
                bootstrap_samples.append(np.mean(sample))
            
            lower = np.percentile(bootstrap_samples, alpha/2 * 100)
            upper = np.percentile(bootstrap_samples, (1 - alpha/2) * 100)
            
            confidence_intervals[alt_name] = (lower, upper)
        
        return confidence_intervals
    
    def _calculate_rank_probabilities(
        self,
        rankings_matrix: np.ndarray
    ) -> pd.DataFrame:
        """ê° ëŒ€ì•ˆì´ íŠ¹ì • ìˆœìœ„ë¥¼ ì°¨ì§€í•  í™•ë¥ """
        
        n_iterations, n_alternatives = rankings_matrix.shape
        rank_probs = np.zeros((n_alternatives, n_alternatives))
        
        for alt_idx in range(n_alternatives):
            for rank in range(1, n_alternatives + 1):
                prob = np.sum(rankings_matrix[:, alt_idx] == rank) / n_iterations
                rank_probs[alt_idx, rank-1] = prob
        
        df = pd.DataFrame(
            rank_probs,
            index=self.model.alternatives,
            columns=[f'Rank_{i+1}' for i in range(n_alternatives)]
        )
        
        return df
    
    def _calculate_rank_stability(
        self,
        rankings_matrix: np.ndarray
    ) -> Dict[str, float]:
        """ìˆœìœ„ ì•ˆì •ì„± ì§€í‘œ"""
        
        stability_scores = {}
        
        for i, alt_name in enumerate(self.model.alternatives):
            ranks = rankings_matrix[:, i]
            
            # ìµœë¹ˆ ìˆœìœ„
            mode_rank = stats.mode(ranks)[0][0]
            mode_frequency = np.sum(ranks == mode_rank) / len(ranks)
            
            # ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ì•ˆì •ì„±
            rank_counts = np.bincount(ranks.astype(int))[1:]  # 0 ì œì™¸
            rank_probs = rank_counts / rank_counts.sum()
            entropy = -np.sum(rank_probs * np.log(rank_probs + 1e-10))
            max_entropy = np.log(len(self.model.alternatives))
            stability = 1 - (entropy / max_entropy)
            
            stability_scores[alt_name] = {
                'mode_rank': mode_rank,
                'mode_frequency': mode_frequency,
                'stability_index': stability
            }
        
        return stability_scores
    
    def _calculate_dominance_matrix(
        self,
        scores_matrix: np.ndarray
    ) -> pd.DataFrame:
        """í™•ë¥ ì  ìš°ìœ„ í–‰ë ¬"""
        
        n_alternatives = scores_matrix.shape[1]
        dominance = np.zeros((n_alternatives, n_alternatives))
        
        for i in range(n_alternatives):
            for j in range(n_alternatives):
                if i != j:
                    # iê°€ jë³´ë‹¤ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì„ í™•ë¥ 
                    dominance[i, j] = np.mean(scores_matrix[:, i] > scores_matrix[:, j])
        
        df = pd.DataFrame(
            dominance,
            index=self.model.alternatives,
            columns=self.model.alternatives
        )
        
        return df
```

### 4.2 ë² ì´ì§€ì•ˆ AHP

```python
import pymc3 as pm
from theano import tensor as tt

class BayesianAHPEngine:
    """ë² ì´ì§€ì•ˆ ì ‘ê·¼ë²•ì„ ì´ìš©í•œ AHP ë¶„ì„"""
    
    def __init__(self, ahp_model):
        self.model = ahp_model
        self.posterior_samples = None
        
    def build_bayesian_model(
        self,
        prior_type: str = 'dirichlet',
        prior_strength: float = 1.0
    ) -> pm.Model:
        """
        ë² ì´ì§€ì•ˆ AHP ëª¨ë¸ êµ¬ì¶•
        
        Parameters:
            prior_type: ì‚¬ì „ ë¶„í¬ ìœ í˜•
            prior_strength: ì‚¬ì „ ë¶„í¬ ê°•ë„
        
        Returns:
            PyMC3 ëª¨ë¸
        """
        
        with pm.Model() as bayesian_ahp:
            
            # ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ì‚¬ì „ ë¶„í¬
            if prior_type == 'dirichlet':
                # Dirichlet ë¶„í¬ (í•©ì´ 1ì¸ ì œì•½)
                alpha = np.ones(self.model.n_criteria) * prior_strength
                weights = pm.Dirichlet('weights', a=alpha)
                
            elif prior_type == 'normal':
                # ì •ê·œ ë¶„í¬ (softmax ë³€í™˜)
                raw_weights = pm.Normal(
                    'raw_weights',
                    mu=0,
                    sigma=prior_strength,
                    shape=self.model.n_criteria
                )
                weights = pm.Deterministic(
                    'weights',
                    tt.nnet.softmax(raw_weights)
                )
            
            # íŒë‹¨ í–‰ë ¬ì˜ ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§
            for matrix_name, observed_matrix in self.model.judgment_matrices.items():
                n = observed_matrix.shape[0]
                
                for i in range(n):
                    for j in range(i+1, n):
                        # Log-normal ë¶„í¬ë¡œ íŒë‹¨ê°’ ëª¨ë¸ë§
                        observed_value = observed_matrix[i, j]
                        log_mean = np.log(observed_value)
                        
                        judgment = pm.Lognormal(
                            f'{matrix_name}_{i}_{j}',
                            mu=log_mean,
                            sigma=0.5,
                            observed=observed_value
                        )
            
            # ìµœì¢… ì ìˆ˜ ê³„ì‚°
            scores = pm.Deterministic(
                'scores',
                self._calculate_scores_theano(weights)
            )
        
        return bayesian_ahp
    
    def run_mcmc(
        self,
        n_samples: int = 5000,
        n_chains: int = 4,
        n_tune: int = 2000
    ) -> Dict[str, any]:
        """
        MCMC ìƒ˜í”Œë§ ì‹¤í–‰
        
        Parameters:
            n_samples: ìƒ˜í”Œ ìˆ˜
            n_chains: ì²´ì¸ ìˆ˜
            n_tune: íŠœë‹ ë‹¨ê³„ ìˆ˜
        
        Returns:
            ì‚¬í›„ ë¶„í¬ í†µê³„
        """
        
        model = self.build_bayesian_model()
        
        with model:
            # NUTS ìƒ˜í”ŒëŸ¬ë¡œ MCMC ì‹¤í–‰
            trace = pm.sample(
                draws=n_samples,
                chains=n_chains,
                tune=n_tune,
                return_inferencedata=True,
                progressbar=True
            )
        
        self.posterior_samples = trace
        
        # ì‚¬í›„ ë¶„í¬ ë¶„ì„
        posterior_stats = {
            'weights_mean': trace.posterior['weights'].mean(dim=['chain', 'draw']).values,
            'weights_std': trace.posterior['weights'].std(dim=['chain', 'draw']).values,
            'weights_hdi': pm.hdi(trace, hdi_prob=0.95)['weights'],
            'scores_mean': trace.posterior['scores'].mean(dim=['chain', 'draw']).values,
            'scores_std': trace.posterior['scores'].std(dim=['chain', 'draw']).values,
            'convergence': self._check_convergence(trace)
        }
        
        return posterior_stats
    
    def _check_convergence(self, trace) -> Dict[str, bool]:
        """MCMC ìˆ˜ë ´ ì§„ë‹¨"""
        
        convergence_checks = {}
        
        # Gelman-Rubin í†µê³„ëŸ‰
        r_hat = pm.rhat(trace)
        convergence_checks['gelman_rubin'] = all(r_hat['weights'] < 1.01)
        
        # Effective sample size
        ess = pm.ess(trace)
        convergence_checks['ess'] = all(ess['weights'] > 1000)
        
        return convergence_checks
```

---

## 5. ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ ë¶„ì„

### 5.1 êµ¬ê°„ AHP (Interval AHP)

```python
class IntervalAHPAnalyzer:
    """êµ¬ê°„ íŒë‹¨ì„ ì´ìš©í•œ ë¡œë²„ìŠ¤íŠ¸ AHP"""
    
    def __init__(self, ahp_model):
        self.model = ahp_model
        
    def analyze_with_intervals(
        self,
        judgment_intervals: Dict[Tuple[int, int], Tuple[float, float]]
    ) -> Dict[str, any]:
        """
        êµ¬ê°„ íŒë‹¨ìœ¼ë¡œ ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ ë¶„ì„
        
        Parameters:
            judgment_intervals: íŒë‹¨ê°’ì˜ í•˜í•œê³¼ ìƒí•œ
        
        Returns:
            êµ¬ê°„ ë¶„ì„ ê²°ê³¼
        """
        
        from scipy.optimize import linprog
        
        # ì„ í˜• ê³„íš ë¬¸ì œë¡œ ë³€í™˜
        # ìµœì†Œ/ìµœëŒ€ ê°€ì¤‘ì¹˜ ì°¾ê¸°
        n_criteria = self.model.n_criteria
        
        # ì œì•½ ì¡°ê±´ í–‰ë ¬ êµ¬ì„±
        A_eq = np.ones((1, n_criteria))  # í•© = 1
        b_eq = np.array([1])
        
        bounds = [(0.01, 1) for _ in range(n_criteria)]
        
        # ê° ê¸°ì¤€ë³„ ìµœì†Œ/ìµœëŒ€ ê°€ì¤‘ì¹˜ ê³„ì‚°
        weight_intervals = {}
        
        for i in range(n_criteria):
            # ìµœì†Œ ê°€ì¤‘ì¹˜
            c = np.zeros(n_criteria)
            c[i] = 1  # ìµœì†Œí™”
            
            result_min = linprog(
                c, 
                A_eq=A_eq, 
                b_eq=b_eq,
                bounds=bounds,
                method='highs'
            )
            
            # ìµœëŒ€ ê°€ì¤‘ì¹˜
            c[i] = -1  # ìµœëŒ€í™” (ìŒìˆ˜ë¡œ ë³€í™˜)
            
            result_max = linprog(
                c,
                A_eq=A_eq,
                b_eq=b_eq,
                bounds=bounds,
                method='highs'
            )
            
            weight_intervals[f'criterion_{i}'] = (
                result_min.x[i],
                -result_max.fun  # ìŒìˆ˜ ì œê±°
            )
        
        # ì ìˆ˜ êµ¬ê°„ ê³„ì‚°
        score_intervals = self._calculate_score_intervals(weight_intervals)
        
        # ìˆœìœ„ ì—­ì „ ê°€ëŠ¥ì„± ë¶„ì„
        reversal_possibilities = self._analyze_rank_reversal_possibility(
            score_intervals
        )
        
        return {
            'weight_intervals': weight_intervals,
            'score_intervals': score_intervals,
            'reversal_possibilities': reversal_possibilities,
            'robustness_measure': self._calculate_interval_robustness(score_intervals)
        }
    
    def _calculate_score_intervals(
        self,
        weight_intervals: Dict[str, Tuple[float, float]]
    ) -> Dict[str, Tuple[float, float]]:
        """ì ìˆ˜ êµ¬ê°„ ê³„ì‚°"""
        
        score_intervals = {}
        performance_matrix = self.model.get_performance_matrix()
        
        for alt_idx, alt_name in enumerate(self.model.alternatives):
            min_score = 0
            max_score = 0
            
            for crit_idx in range(self.model.n_criteria):
                perf = performance_matrix[alt_idx, crit_idx]
                w_min, w_max = weight_intervals[f'criterion_{crit_idx}']
                
                # ì„±ëŠ¥ì´ ì–‘ìˆ˜ë©´ ìµœëŒ€ ê°€ì¤‘ì¹˜ ì‚¬ìš©
                if perf >= 0:
                    min_score += perf * w_min
                    max_score += perf * w_max
                else:
                    min_score += perf * w_max
                    max_score += perf * w_min
            
            score_intervals[alt_name] = (min_score, max_score)
        
        return score_intervals
```

---

## 6. í†µí•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ

### 6.1 ë¶„ì„ ê²°ê³¼ í†µí•©

```python
class AdvancedAnalysisOrchestrator:
    """ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ í†µí•© ê´€ë¦¬"""
    
    def __init__(self, ahp_model):
        self.model = ahp_model
        self.local_sensitivity = LocalSensitivityAnalyzer(ahp_model)
        self.global_sensitivity = GlobalSensitivityAnalyzer(ahp_model)
        self.whatif_engine = WhatIfScenarioEngine(ahp_model)
        self.monte_carlo = MonteCarloAHPEngine(ahp_model)
        self.bayesian = BayesianAHPEngine(ahp_model)
        self.interval_ahp = IntervalAHPAnalyzer(ahp_model)
        
    def run_comprehensive_analysis(
        self,
        analyses: List[str] = None
    ) -> Dict[str, any]:
        """
        ì¢…í•©ì ì¸ ê³ ê¸‰ ë¶„ì„ ì‹¤í–‰
        
        Parameters:
            analyses: ì‹¤í–‰í•  ë¶„ì„ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            í†µí•© ë¶„ì„ ê²°ê³¼
        """
        
        if analyses is None:
            analyses = [
                'local_sensitivity',
                'global_sensitivity',
                'scenarios',
                'monte_carlo',
                'robustness'
            ]
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'model_info': self._get_model_info()
        }
        
        # 1. ë¯¼ê°ë„ ë¶„ì„
        if 'local_sensitivity' in analyses:
            results['local_sensitivity'] = self._run_local_sensitivity()
        
        if 'global_sensitivity' in analyses:
            results['global_sensitivity'] = self._run_global_sensitivity()
        
        # 2. ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
        if 'scenarios' in analyses:
            results['scenarios'] = self._run_scenario_analysis()
        
        # 3. Monte Carlo ì‹œë®¬ë ˆì´ì…˜
        if 'monte_carlo' in analyses:
            results['monte_carlo'] = self._run_monte_carlo()
        
        # 4. ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ ë¶„ì„
        if 'robustness' in analyses:
            results['robustness'] = self._run_robustness_analysis()
        
        # 5. í†µí•© ê¶Œê³ ì‚¬í•­ ìƒì„±
        results['recommendations'] = self._generate_recommendations(results)
        
        return results
    
    def _generate_recommendations(
        self,
        analysis_results: Dict[str, any]
    ) -> List[Dict[str, str]]:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ë¯¼ê°ë„ ê¸°ë°˜ ê¶Œê³ 
        if 'local_sensitivity' in analysis_results:
            sensitive_criteria = self._identify_sensitive_criteria(
                analysis_results['local_sensitivity']
            )
            
            if sensitive_criteria:
                recommendations.append({
                    'type': 'sensitivity',
                    'priority': 'high',
                    'message': f"ê¸°ì¤€ {sensitive_criteria}ì— ëŒ€í•œ íŒë‹¨ì„ ì‹ ì¤‘íˆ ê²€í† í•˜ì„¸ìš”.",
                    'action': 'review_judgments'
                })
        
        # Monte Carlo ê¸°ë°˜ ê¶Œê³ 
        if 'monte_carlo' in analysis_results:
            uncertain_alternatives = self._identify_uncertain_alternatives(
                analysis_results['monte_carlo']
            )
            
            if uncertain_alternatives:
                recommendations.append({
                    'type': 'uncertainty',
                    'priority': 'medium',
                    'message': f"ëŒ€ì•ˆ {uncertain_alternatives}ì˜ ìˆœìœ„ê°€ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤.",
                    'action': 'collect_more_data'
                })
        
        # ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ ê¸°ë°˜ ê¶Œê³ 
        if 'robustness' in analysis_results:
            robustness_score = analysis_results['robustness']['overall_score']
            
            if robustness_score < 0.7:
                recommendations.append({
                    'type': 'robustness',
                    'priority': 'high',
                    'message': "ê²°ê³¼ì˜ ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ê°€ ë‚®ìŠµë‹ˆë‹¤. ì¶”ê°€ ë¶„ì„ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
                    'action': 'additional_analysis'
                })
        
        return recommendations
    
    def generate_report(
        self,
        results: Dict[str, any],
        format: str = 'html'
    ) -> str:
        """
        ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
        
        Parameters:
            results: ë¶„ì„ ê²°ê³¼
            format: ì¶œë ¥ í˜•ì‹ ('html', 'pdf', 'markdown')
        
        Returns:
            ë³´ê³ ì„œ ë‚´ìš©
        """
        
        if format == 'html':
            return self._generate_html_report(results)
        elif format == 'pdf':
            return self._generate_pdf_report(results)
        elif format == 'markdown':
            return self._generate_markdown_report(results)
        else:
            raise ValueError(f"Unsupported format: {format}")
```

---

## 7. ì„±ëŠ¥ ìµœì í™”

### 7.1 ê³„ì‚° ìµœì í™”

```python
import numba
from joblib import Memory

# ìºì‹± ì„¤ì •
memory = Memory(location='/tmp/ahp_cache', verbose=0)

@numba.jit(nopython=True, parallel=True)
def fast_eigenvalue_calculation(matrix: np.ndarray) -> Tuple[float, np.ndarray]:
    """Numbaë¥¼ ì´ìš©í•œ ê³ ì† ê³ ìœ ê°’ ê³„ì‚°"""
    
    n = matrix.shape[0]
    max_iter = 100
    tolerance = 1e-10
    
    # Power method
    v = np.ones(n) / n
    
    for _ in range(max_iter):
        v_new = matrix @ v
        v_new = v_new / np.linalg.norm(v_new)
        
        if np.abs(np.linalg.norm(v - v_new)) < tolerance:
            break
        
        v = v_new
    
    eigenvalue = np.sum((matrix @ v) / v) / n
    
    return eigenvalue, v

@memory.cache
def cached_sobol_analysis(problem: dict, n_samples: int) -> dict:
    """ìºì‹œëœ Sobol ë¶„ì„"""
    # ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ ìºì‹œëœ ê²°ê³¼ ë°˜í™˜
    pass

class OptimizedAnalysisEngine:
    """ìµœì í™”ëœ ë¶„ì„ ì—”ì§„"""
    
    def __init__(self):
        self.use_gpu = self._check_gpu_availability()
        
    def _check_gpu_availability(self) -> bool:
        """GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import cupy as cp
            return True
        except ImportError:
            return False
    
    def optimized_monte_carlo(
        self,
        n_iterations: int,
        use_gpu: bool = None
    ) -> np.ndarray:
        """GPU ê°€ì† Monte Carlo"""
        
        if use_gpu is None:
            use_gpu = self.use_gpu
        
        if use_gpu:
            import cupy as cp
            # GPUì—ì„œ ê³„ì‚°
            random_matrix = cp.random.random((n_iterations, self.n_criteria))
            results = self._gpu_calculation(random_matrix)
            return cp.asnumpy(results)
        else:
            # CPUì—ì„œ ë³‘ë ¬ ê³„ì‚°
            return self._parallel_cpu_calculation(n_iterations)
```

---

## 8. í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### 8.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
import unittest
import numpy.testing as npt

class TestAdvancedAnalysis(unittest.TestCase):
    """ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.test_model = self._create_test_model()
        self.analyzer = AdvancedAnalysisOrchestrator(self.test_model)
    
    def test_sensitivity_analysis(self):
        """ë¯¼ê°ë„ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        local_sens = LocalSensitivityAnalyzer(self.test_model)
        
        # íƒ„ë ¥ì„± ê³„ì‚° í…ŒìŠ¤íŠ¸
        elasticity = local_sens.calculate_elasticity(0)
        
        self.assertIsInstance(elasticity, dict)
        self.assertEqual(len(elasticity), len(self.test_model.alternatives))
        
        # ê°’ ë²”ìœ„ í™•ì¸
        for value in elasticity.values():
            self.assertGreaterEqual(value, -10)
            self.assertLessEqual(value, 10)
    
    def test_monte_carlo_convergence(self):
        """Monte Carlo ìˆ˜ë ´ í…ŒìŠ¤íŠ¸"""
        mc_engine = MonteCarloAHPEngine(self.test_model)
        
        # ì‘ì€ ìƒ˜í”Œê³¼ í° ìƒ˜í”Œ ë¹„êµ
        results_small = mc_engine.run_simulation(n_iterations=100)
        results_large = mc_engine.run_simulation(n_iterations=10000)
        
        # í° ìƒ˜í”Œì˜ ë¶„ì‚°ì´ ë” ì‘ì•„ì•¼ í•¨
        self.assertLess(
            results_large['std_scores'].mean(),
            results_small['std_scores'].mean()
        )
    
    def test_scenario_generation(self):
        """ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± í…ŒìŠ¤íŠ¸"""
        whatif = WhatIfScenarioEngine(self.test_model)
        
        scenarios = whatif.generate_automatic_scenarios()
        
        self.assertGreater(len(scenarios), 0)
        
        for scenario in scenarios:
            # ì‹œë‚˜ë¦¬ì˜¤ í‰ê°€ ê°€ëŠ¥í•œì§€ í™•ì¸
            result = whatif.evaluate_scenario(scenario)
            self.assertIn('final_scores', result)
            self.assertIn('ranking', result)
    
    def test_robustness_measures(self):
        """ë¡œë²„ìŠ¤íŠ¸ë‹ˆìŠ¤ ì¸¡ì • í…ŒìŠ¤íŠ¸"""
        interval_analyzer = IntervalAHPAnalyzer(self.test_model)
        
        # í…ŒìŠ¤íŠ¸ êµ¬ê°„ ì„¤ì •
        test_intervals = {
            (0, 1): (3, 5),
            (0, 2): (2, 4),
            (1, 2): (0.5, 2)
        }
        
        results = interval_analyzer.analyze_with_intervals(test_intervals)
        
        self.assertIn('weight_intervals', results)
        self.assertIn('score_intervals', results)
        self.assertIn('robustness_measure', results)
```

---

## 9. êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ (Week 1)
- [ ] êµ­ì†Œ ë¯¼ê°ë„ ë¶„ì„ êµ¬í˜„
- [ ] íƒ„ë ¥ì„± ê³„ì‚°
- [ ] ìˆœìœ„ ì—­ì „ í¬ì¸íŠ¸ ì°¾ê¸°
- [ ] ê¸°ë³¸ í…ŒìŠ¤íŠ¸

### Phase 2: ì‹œë‚˜ë¦¬ì˜¤ ì—”ì§„ (Week 2)
- [ ] What-if ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
- [ ] ì‹œë‚˜ë¦¬ì˜¤ í‰ê°€ ë° ë¹„êµ
- [ ] ì‹œë‚˜ë¦¬ì˜¤ ìµœì í™”
- [ ] ì‹œë‚˜ë¦¬ì˜¤ í´ëŸ¬ìŠ¤í„°ë§

### Phase 3: Monte Carlo (Week 3)
- [ ] í™•ë¥ ì  ëª¨ë¸ë§
- [ ] ë³‘ë ¬ ì‹œë®¬ë ˆì´ì…˜
- [ ] ê²°ê³¼ í†µê³„ ë¶„ì„
- [ ] ë² ì´ì§€ì•ˆ ì ‘ê·¼ë²•

### Phase 4: í†µí•© ë° ìµœì í™” (Week 4)
- [ ] í†µí•© ëŒ€ì‹œë³´ë“œ
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] GPU ê°€ì†
- [ ] ë³´ê³ ì„œ ìƒì„±

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ìˆ˜ì •ì¼**: 2024-11-29  
**ì‘ì„±ì**: Claude Opus 4.1  
**ê²€í†  ìƒíƒœ**: ì„¤ê³„ ì™„ë£Œ